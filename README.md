## A new way to create embeddings for code search, inspired by the ConceptFusion paper.

The meat of the idea is in the get_local_embeddings function, which is my implementation of section 4.2 of the ConceptFusion paper (they don't have code released yet). It gives a weighted combination of the global embedding of a piece of code, which I chose to be the entire code plus the README, and the embeddings of subsets of the code. The idea is similar to attention: the weight for a local chunk of code is determined by its similarity with the neighboring chunks and with global information. They did it on images, but I thought this incorporation of the "uniqueness" of local embeddings was applicable to natural language as well. For demonstration purposes, I averaged the cosine similarities between the query and the local embeddings, and the result is better than naive cosine similarity on average even without tuning, with the additional advantage of being able to extract fine-grained information without requiring additional labeling. Still, it might be interesting to provide more fine-grained labels on local chunks of code and further test its ability to return context-aware and relevant information. 
